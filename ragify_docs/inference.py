"""
Retrieval-Augmented Generation (RAG) inference controller.

Handles document retrieval from a Chroma vector store
and uses either OpenAI or Ollama to generate answers based on the retrieved context.
"""

import logging
import os

from ragify_docs.config import settings
from ragify_docs.core.store import ChromaStore
from ragify_docs.core.openai import call_openai
from ragify_docs.core.ollama import call_ollama

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

os.environ["TOKENIZERS_PARALLELISM"] = str(settings.tokenizers_parallelism)


class LLMRetriever:

    def __init__(self):
        """
        Initialize the RAG controller with a Chroma vector store.
        """
        self.store = ChromaStore()

    def generate_prompt(self, question: str, top_k: int = 5):
        """
        Generate a prompt for the LLM by querying the Chroma vector store for top-K documents.

        Args:
            question (str): User question.
            top_k (int, optional): Number of documents to retrieve. Defaults to 5.

        Returns:
            str: Prompt with retrieved context.
        """
        logger.info(f"Querying for: {question}")
        results = self.store.query(question, top_k)
        logger.info(f"Retrieved {len(results)} documents for query: '{question}'")

        context_text = "\n\n".join(results) if isinstance(results, list) else str(results)
        prompt = (
            f"Use the following context to answer the question:"
            f"\n\n{context_text}\n\nQuestion: {question}\nAnswer:"
        )
        return prompt

    def ask(self, question: str) -> str:
        """
        Ask the LLM a question using the generated prompt.

        Args:
            question (str): User question.

        Returns:
            str: Answer generated by the LLM.
        """
        prompt = self.generate_prompt(question)
        try:
            if settings.use_ollama:
                logger.info("Calling Ollama model: %s", settings.ollama_model)
                return call_ollama(prompt)
            else:
                logger.info("Calling OpenAI model: %s", settings.openai_model)
                return call_openai(prompt)
        except Exception as e:
            logger.exception("Error during LLM call: %s", str(e))
            return "An error occurred while generating a response. Please try again."


def generate_answer(question: str) -> str:
    """
    Convenience function to generate an answer for a given question.

    Args:
        question (str): User question.

    Returns:
        str: Answer generated by the LLM.
    """
    retriever = LLMRetriever()
    return retriever.ask(question)


if __name__ == "__main__":
    print(generate_answer("What is extended inquiry data format?"))
