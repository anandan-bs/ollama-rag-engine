# ðŸ“„ ragify-docs â€” Domain-Aware Chat with Your Own Documents

**ragify-docs** is a RAG (Retrieval-Augmented Generation) powered AI assistant that lets you upload your documents, index their content locally, and ask questions with full context awareness using hybrid LLMs â€” either OpenAI or Ollama.

---

## ðŸš€ Why Use ragify-docs Instead of Public LLM Chat?

### âœ… Traditional Chatbots (like ChatGPT or Claude):
- Do **not remember your documents** unless you paste content manually.
- **Consume more tokens**, especially when repeating context in each query.
- **Do not fine-tune or remember** past documents unless part of ongoing sessions.
- Your inputs are sometimes **used for model improvement** (unless settings are disabled).

### âœ… ragify-docs with RAG:
- ðŸ§  **Injects context** from your indexed documents dynamically â€” no retraining required.
- ðŸ”’ **Runs locally with Ollama**, or optionally uses OpenAI (no document content sent to OpenAI APIs).
- ðŸ’¸ **Reduces token usage** with smart chunking and retrieval.
- ðŸ“‚ Supports **PDFs, DOCX, TXT, MD**, and **Google Docs**.
- ðŸ“ˆ OpenAI Completions API guarantees your content is **not used for training** when using API keys.

---

## ðŸ“¦ Features

- ðŸ“„ Ingest your private documents into a local vector store (ChromaDB)
- ðŸ¤– Ask natural language questions and get answers from context
- ðŸ” Session history with saving and loading
- ðŸ’¬ Hybrid LLM inference (Ollama local or OpenAI fallback)
- ðŸ§  Smart embedding fallback (Ollama or SentenceTransformer)
- ðŸ“¤ Export conversations as Markdown or TXT

---

## ðŸ› ï¸ Installation

```bash
git clone https://github.com/your-username/ragify-docs.git
cd ragify-docs

# Install dependencies
pip install -r requirements.txt

# or using poetry
poetry install
```

---

## âš™ï¸ Configuration

Create a `.env` file (or pass env vars directly):

```env
OPENAI_API_KEY=your_openai_api_key
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3
OLLAMA_EMBED_MODEL=nomic-embed-text
```

These can also be overridden in `config.py` using `pydantic.BaseSettings`.

---

## ðŸ§© Directory Structure

```
project_root/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ config.py        # environment & model settings
â”‚   â”œâ”€â”€ embedder.py      # local embedding logic
â”‚   â”œâ”€â”€ retriever.py     # Chroma vector DB querying
â”‚   â”œâ”€â”€ ingest.py        # parse, chunk & embed documents
â”‚   â”œâ”€â”€ reranker.py      # optional semantic reranking
â”‚   â”œâ”€â”€ inference.py     # RAG controller logic
â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”œâ”€â”€ openai.py    # OpenAI API wrapper
â”‚   â”‚   â””â”€â”€ ollama.py    # Ollama API wrapper
â”œâ”€â”€ sessions/            # saved chat logs
â”œâ”€â”€ exports/             # exported chat sessions
â”œâ”€â”€ chroma_storage/      # persisted Chroma vector data
â”œâ”€â”€ app.py               # Gradio UI frontend
â”œâ”€â”€ requirements.txt
â””â”€â”€ pyproject.toml
```

---

## ðŸ§ª How It Works (Workflow)

```mermaid
flowchart TD
    A[User Uploads Document] --> B[Text is Chunked & Embedded]
    B --> C[Stored in Chroma Vector Store]
    D[User Asks Question] --> E[Embed Query & Retrieve Top-K]
    E --> F[Optional Reranking]
    F --> G[LLM Prompt: Uses Retrieved Chunks as Context]
    C --> G
    G --> H[Answer Displayed via Gradio UI]
```

---

## ðŸš€ Running the App

```bash
# Navigate to the project root (if not already there)
cd path/to/ragify-docs

# Launch the Gradio app
python app.py
```

Then open your browser at [http://localhost:7860](http://localhost:7860)

---

## ðŸ“Œ Notes

- Ollama must be installed and running if `use_ollama = True`
- OpenAI completions use API key securely and **do not send full documents**
- Chroma vector DB and document indexing happen **entirely locally**

---

## ðŸ‘¥ Author

**Anandan B S**
- Email: anandanklnce@gmail.com
- GitHub: [@anandan-bs](https://github.com/anandan-bs)

---

## ðŸ“œ License

MIT License
