# ðŸ“„ ragify-docs â€” Domain-Aware Chat with Your Own Documents
[![CI Status](https://github.com/anandan-bs/ragify-docs/actions/workflows/ci.yml/badge.svg)](https://github.com/anandan-bs/ragify-docs/actions/workflows/main.yml)

**ragify-docs** is a RAG (Retrieval-Augmented Generation) powered AI assistant that lets you upload your documents, index their content locally, and ask questions with full context awareness using hybrid LLMs â€” either OpenAI or Ollama.

---

## ðŸš€ Why Use ragify-docs Instead of Public LLM Chat?

### âœ… Traditional Chatbots (like ChatGPT or Claude):
- Do **not remember your documents** unless you paste content manually.
- **Consume more tokens**, especially when repeating context in each query.
- **Do not fine-tune or remember** past documents unless part of ongoing sessions.
- Your inputs are sometimes **used for model improvement** (unless settings are disabled).

### âœ… ragify-docs with RAG:
- ðŸ§  **Injects context** from your indexed documents dynamically â€” no retraining required.
- ðŸ”’ **Runs locally with Ollama**, or optionally uses OpenAI (no document content sent to OpenAI APIs).
- ðŸ’¸ **Reduces token usage** with smart chunking and retrieval.
- ðŸ“‚ Supports **PDFs, DOCX, TXT, MD**, and **Google Docs**.
- ðŸ“ˆ OpenAI Completions API guarantees your content is **not used for training** when using API keys.

---

## ðŸ“¦ Features

- ðŸ“„ Ingest your private documents into a local vector store (ChromaDB)
- ðŸ¤– Ask natural language questions and get answers from context
- ðŸ” Session history with saving and loading
- ðŸ’¬ Hybrid LLM inference (Ollama local or OpenAI fallback)
- ðŸ§  Smart embedding fallback (Ollama or SentenceTransformer)
- ðŸ“¤ Export conversations as Markdown or TXT

---

## ðŸ› ï¸ Installation

```bash
git clone https://github.com/anandan-bs/ragify-docs.git
cd ragify-docs

# Install dependencies
pip install -r requirements.txt

# or using make
make setup
```

---

## âš™ï¸ Configuration

Create a `.env` file (or pass env vars directly):

```env
# Required for OpenAI API access
OPENAI_API_KEY=your_openai_api_key

# Required for Hugging Face model downloads
HUGGINGFACE_TOKEN=your_huggingface_token

# Optional: Set to 'false' to disable tokenizer parallelism
TOKENIZERS_PARALLELISM='false'
```

These can also be overridden in `ragify_docs/config.py` using `pydantic.BaseSettings`.

---

## ðŸ§© Directory Structure

```
ragify-docs/
â”œâ”€â”€ ragify_docs/
â”‚   â”œâ”€â”€ core/                  # Core functionality modules
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ chunk.py          # Text chunking utilities
â”‚   â”‚   â”œâ”€â”€ embed.py          # Embedding functionality
â”‚   â”‚   â”œâ”€â”€ load_doc.py       # Document loading utilities
â”‚   â”‚   â”œâ”€â”€ ollama.py         # Ollama API integration
â”‚   â”‚   â”œâ”€â”€ openai.py         # OpenAI API integration
â”‚   â”‚   â”œâ”€â”€ store.py          # Vector store operations
â”‚   â”‚   â””â”€â”€ tokenize.py       # Tokenization utilities
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py             # Application configuration
â”‚   â”œâ”€â”€ inference.py          # RAG inference logic
â”‚   â”œâ”€â”€ ingest.py             # Document ingestion pipeline
â”‚   â””â”€â”€ main.py               # Gradio UI and application entry point
â”œâ”€â”€ .data/                    # Local model storage, auto-generated
â”œâ”€â”€ Makefile                  # Development commands
â”œâ”€â”€ README.md                 # This file
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ setup.py                  # Package installation
â””â”€â”€ setup_model.py            # Model setup script
```

---

## ðŸ§ª How It Works (Workflow)

```mermaid
flowchart TD
    %% Central Chroma DB
    DB[(Chroma Vector DB)]
    
    %% Document Ingestion Pipeline
    A[User Uploads Document] --> B[Load Document]
    B --> C[Chunk with Token Awareness]
    C --> D[Generate Embeddings]
    D --> DB
    
    %% Query Processing Pipeline
    E[User Asks Question] --> F[Embed Question]
    F --> G[Query Chroma DB]
    DB -->|Retrieve| G
    G --> H[Generate LLM Prompt with Context]
    H --> I[Display Answer in Gradio UI]
    
    %% Styling for better visualization
    classDef db fill:#4db6ac,stroke:#333,stroke-width:2px,color:white
    class DB db
```

---

## ðŸš€ Running the App

```bash
# Navigate to the project root (if not already there)
cd ragify-docs
python3 setup_model.py
python3 ragify_docs.py


# or using make
make run
```

Then open your browser at [http://localhost:7860](http://localhost:7860)

---

## ðŸ“Œ Notes

- Ollama must be installed and running if `use_ollama = True`
- OpenAI completions use API key securely and **do not send full documents**
- Chroma vector DB and document indexing happen **entirely locally**

---

## ðŸ‘¥ Author

**Anandan B S**
- Email: anandanklnce@gmail.com
- GitHub: [@anandan-bs](https://github.com/anandan-bs)

---

## ðŸ“œ License

MIT License
